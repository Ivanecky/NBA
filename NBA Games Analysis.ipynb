{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Games Analysis\n",
    "The goal of this project is to build a handful of different models to make a predictive model for NBA games. The dataset used has four seasons of data from 2014-2018, not including playoff or preseason games. This data is available publicly via Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Data is loaded from local file that was downloaded from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:25:15.562131Z",
     "start_time": "2020-02-19T19:25:11.918753Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, mean_squared_error\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:25:20.340067Z",
     "start_time": "2020-02-19T19:25:20.298754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "nba = pd.read_csv(\"nba.games.stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:59:51.692393Z",
     "start_time": "2020-02-19T19:59:51.547916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in ELO data (from 538)\n",
    "nba_elo = pd.read_csv(\"nba_elo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Next steps will be to clean up the data and create some new features that may be useful for future predcitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:30.579639Z",
     "start_time": "2020-02-19T14:10:30.562957Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out data information\n",
    "nba.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:26:21.007436Z",
     "start_time": "2020-02-19T19:26:20.983726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69635 entries, 0 to 69634\n",
      "Data columns (total 24 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            69635 non-null  object \n",
      " 1   season          69635 non-null  int64  \n",
      " 2   neutral         69635 non-null  int64  \n",
      " 3   playoff         4362 non-null   object \n",
      " 4   team1           69635 non-null  object \n",
      " 5   team2           69635 non-null  object \n",
      " 6   elo1_pre        69635 non-null  float64\n",
      " 7   elo2_pre        69635 non-null  float64\n",
      " 8   elo_prob1       69635 non-null  float64\n",
      " 9   elo_prob2       69635 non-null  float64\n",
      " 10  elo1_post       69224 non-null  float64\n",
      " 11  elo2_post       69224 non-null  float64\n",
      " 12  carm-elo1_pre   6478 non-null   float64\n",
      " 13  carm-elo2_pre   6478 non-null   float64\n",
      " 14  carm-elo_prob1  6478 non-null   float64\n",
      " 15  carm-elo_prob2  6478 non-null   float64\n",
      " 16  carm-elo1_post  6067 non-null   float64\n",
      " 17  carm-elo2_post  6067 non-null   float64\n",
      " 18  raptor1_pre     2541 non-null   float64\n",
      " 19  raptor2_pre     2541 non-null   float64\n",
      " 20  raptor_prob1    2541 non-null   float64\n",
      " 21  raptor_prob2    2541 non-null   float64\n",
      " 22  score1          69224 non-null  float64\n",
      " 23  score2          69224 non-null  float64\n",
      "dtypes: float64(18), int64(2), object(4)\n",
      "memory usage: 12.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Print out ELO info\n",
    "nba_elo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:26:42.324620Z",
     "start_time": "2020-02-19T19:26:42.310461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Most of the above are ints or floats, but a few objects should be categories.\n",
    "# Next, convert these to categories.\n",
    "nba['Team'] = nba['Team'].astype('category')\n",
    "nba['Home'] = nba['Home'].astype('category')\n",
    "nba['Opponent'] = nba['Opponent'].astype('category')\n",
    "nba['WINorLOSS'] = nba['WINorLOSS'].astype('category')\n",
    "# Convert date to a date object\n",
    "nba['Date'] = pd.to_datetime(nba['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:26:44.443332Z",
     "start_time": "2020-02-19T19:26:44.427908Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9840 entries, 0 to 9839\n",
      "Data columns (total 41 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   Unnamed: 0                9840 non-null   int64         \n",
      " 1   Team                      9840 non-null   category      \n",
      " 2   Game                      9840 non-null   int64         \n",
      " 3   Date                      9840 non-null   datetime64[ns]\n",
      " 4   Home                      9840 non-null   category      \n",
      " 5   Opponent                  9840 non-null   category      \n",
      " 6   WINorLOSS                 9840 non-null   category      \n",
      " 7   TeamPoints                9840 non-null   int64         \n",
      " 8   OpponentPoints            9840 non-null   int64         \n",
      " 9   FieldGoals                9840 non-null   int64         \n",
      " 10  FieldGoalsAttempted       9840 non-null   int64         \n",
      " 11  FieldGoals.               9840 non-null   float64       \n",
      " 12  X3PointShots              9840 non-null   int64         \n",
      " 13  X3PointShotsAttempted     9840 non-null   int64         \n",
      " 14  X3PointShots.             9840 non-null   float64       \n",
      " 15  FreeThrows                9840 non-null   int64         \n",
      " 16  FreeThrowsAttempted       9840 non-null   int64         \n",
      " 17  FreeThrows.               9840 non-null   float64       \n",
      " 18  OffRebounds               9840 non-null   int64         \n",
      " 19  TotalRebounds             9840 non-null   int64         \n",
      " 20  Assists                   9840 non-null   int64         \n",
      " 21  Steals                    9840 non-null   int64         \n",
      " 22  Blocks                    9840 non-null   int64         \n",
      " 23  Turnovers                 9840 non-null   int64         \n",
      " 24  TotalFouls                9840 non-null   int64         \n",
      " 25  Opp.FieldGoals            9840 non-null   int64         \n",
      " 26  Opp.FieldGoalsAttempted   9840 non-null   int64         \n",
      " 27  Opp.FieldGoals.           9840 non-null   float64       \n",
      " 28  Opp.3PointShots           9840 non-null   int64         \n",
      " 29  Opp.3PointShotsAttempted  9840 non-null   int64         \n",
      " 30  Opp.3PointShots.          9840 non-null   float64       \n",
      " 31  Opp.FreeThrows            9840 non-null   int64         \n",
      " 32  Opp.FreeThrowsAttempted   9840 non-null   int64         \n",
      " 33  Opp.FreeThrows.           9840 non-null   float64       \n",
      " 34  Opp.OffRebounds           9840 non-null   int64         \n",
      " 35  Opp.TotalRebounds         9840 non-null   int64         \n",
      " 36  Opp.Assists               9840 non-null   int64         \n",
      " 37  Opp.Steals                9840 non-null   int64         \n",
      " 38  Opp.Blocks                9840 non-null   int64         \n",
      " 39  Opp.Turnovers             9840 non-null   int64         \n",
      " 40  Opp.TotalFouls            9840 non-null   int64         \n",
      "dtypes: category(4), datetime64[ns](1), float64(6), int64(30)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Confirm the changes worked\n",
    "nba.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:59:55.558988Z",
     "start_time": "2020-02-19T19:59:55.553158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out ELO data < 2015 and > 2018\n",
    "nba_elo = nba_elo[((nba_elo['season'] >= 2015) & (nba_elo['season'] < 2019))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:59:56.355145Z",
     "start_time": "2020-02-19T19:59:56.351200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to only a few needed columns\n",
    "nba_elo = nba_elo[['date','team1','team2','elo1_pre','elo2_pre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:59:57.164999Z",
     "start_time": "2020-02-19T19:59:57.159813Z"
    }
   },
   "outputs": [],
   "source": [
    "nba_elo['date'] = pd.to_datetime(nba_elo['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create season variable\n",
    "There are four distinct seasons, with no games in a season coming after May 1st of that year. Let's categorize every game for particular seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:34:56.738215Z",
     "start_time": "2020-02-19T19:34:54.786231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def getSeason(x):\n",
    "    if x < pd.to_datetime('2015-5-1'):\n",
    "        return '14-15'\n",
    "    elif x < pd.to_datetime('2016-5-1'):\n",
    "        return '15-16'\n",
    "    elif x < pd.to_datetime('2017-5-1'):\n",
    "        return '16-17'\n",
    "    else:\n",
    "        return '17-18'\n",
    "\n",
    "nba['season'] = nba['Date'].apply(getSeason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:34:57.946041Z",
     "start_time": "2020-02-19T19:34:57.938051Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17-18    2460\n",
       "16-17    2460\n",
       "15-16    2460\n",
       "14-15    2460\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to category\n",
    "nba['season'] = nba['season'].astype('category')\n",
    "# Check results\n",
    "nba['season'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get running total of wins & losses\n",
    "Need to calculate the running total of wins and losses for the home and away teams to calculate winning pct for a certain point in time.\n",
    "\n",
    "Logical approach seems to be create a separate dataframe with each teams wins, losses and win pct for each date and then join this table (twice) to the main table. One join would bring in the Home teams info and the second join would tie in the Away teams info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:32.480648Z",
     "start_time": "2020-02-19T14:10:32.444867Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(nba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:16.066994Z",
     "start_time": "2020-02-19T19:35:16.053316Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create copy of nba dataframe\n",
    "temp = nba\n",
    "# Define a numeric W/L column for cumulative sum\n",
    "temp['win_loss'] = np.where(temp['WINorLOSS'] == 'W', 1, 0)\n",
    "# Create four subsets - one for each season\n",
    "# Might be a better way to do this but sticking with simplicity for now\n",
    "s1 = temp[temp['season'] == '14-15']\n",
    "s2 = temp[temp['season'] == '15-16']\n",
    "s3 = temp[temp['season'] == '16-17']\n",
    "s4 = temp[temp['season'] == '17-18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:17.141601Z",
     "start_time": "2020-02-19T19:35:17.133133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to do all of the below\n",
    "def getRollingData(df):\n",
    "    # Create a temporary dataframe\n",
    "    temp = df\n",
    "    # Get rolling win total\n",
    "    df_wins = df.groupby(['Team','Game','Date']).win_loss.sum().groupby(level=[0]).cumsum() # Get rolling win total\n",
    "    df_wins = pd.DataFrame(df_wins).reset_index() # Removes multi index from grouping\n",
    "    df_wins = df_wins[np.isfinite(df_wins['win_loss'])] # Remove NaN rows\n",
    "    opp_wins = df_wins.rename(columns = {'win_loss': 'opp_wins', 'Game': 'opp_games'}) # Opponent data\n",
    "    df_wins = df_wins.rename(columns = {'win_loss': 'wins','Game': 'home_game'}) # Rename cols for merge\n",
    "    df = df.merge(df_wins, on = ['Team','Date'], how = 'left') # Join back to original season data\n",
    "    df = df.merge(opp_wins, on =['Team','Date'], how = 'left') # Join in opponent info\n",
    "    # Only keep a select few columns\n",
    "    df = df[['Team','Game','Date','Home','Opponent','WINorLOSS','wins','home_game','opp_wins','opp_games']]\n",
    "    # Get rolling averages\n",
    "    temp = temp.drop(columns = {'Home','Opponent','WINorLOSS','season','win_loss','Date'})\n",
    "    temp = temp.groupby(['Team']).expanding().mean().reset_index() # Gets cumulative avg of numeric cols\n",
    "    temp['Game'] = temp['level_1'] % 82 + 2 # This fixes the game column (for joining). Index offset which means the avg for a game is one behind the game which is ideal\n",
    "    df = df.merge(temp, on = ['Team', 'Game'], how = 'left')\n",
    "    df = df.drop(columns = ['level_1','Unnamed: 0'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:33.318514Z",
     "start_time": "2020-02-19T19:35:19.753445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the seasons data\n",
    "season1 = getRollingData(s1)\n",
    "season2 = getRollingData(s2)\n",
    "season3 = getRollingData(s3)\n",
    "season4 = getRollingData(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:35.148013Z",
     "start_time": "2020-02-19T19:35:35.132574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine all the data\n",
    "nba = pd.concat([season1, season2, season3, season4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:58:04.620265Z",
     "start_time": "2020-02-19T19:58:04.615911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'team1', 'team2', 'elo1_pre', 'elo2_pre'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_elo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T20:00:12.273860Z",
     "start_time": "2020-02-19T20:00:12.269987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename ELO columns for join\n",
    "nba_elo = nba_elo.rename(columns={'date': 'Date', 'team1': 'Team', 'team2': 'Opponent', \n",
    "                                  'elo1_pre': 'elo', 'elo2_pre': 'elo_opp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T20:00:58.784275Z",
     "start_time": "2020-02-19T20:00:58.760563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Join in ELO ratings\n",
    "temp = pd.merge(nba, nba_elo, how='left', on = ['Date','Team','Opponent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T20:01:32.952416Z",
     "start_time": "2020-02-19T20:01:32.925594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Game</th>\n",
       "      <th>Date</th>\n",
       "      <th>Home</th>\n",
       "      <th>Opponent</th>\n",
       "      <th>WINorLOSS</th>\n",
       "      <th>wins</th>\n",
       "      <th>home_game</th>\n",
       "      <th>opp_wins</th>\n",
       "      <th>opp_games</th>\n",
       "      <th>...</th>\n",
       "      <th>Opp.FreeThrows.</th>\n",
       "      <th>Opp.OffRebounds</th>\n",
       "      <th>Opp.TotalRebounds</th>\n",
       "      <th>Opp.Assists</th>\n",
       "      <th>Opp.Steals</th>\n",
       "      <th>Opp.Blocks</th>\n",
       "      <th>Opp.Turnovers</th>\n",
       "      <th>Opp.TotalFouls</th>\n",
       "      <th>elo</th>\n",
       "      <th>elo_opp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-29</td>\n",
       "      <td>Away</td>\n",
       "      <td>TOR</td>\n",
       "      <td>L</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-11-01</td>\n",
       "      <td>Home</td>\n",
       "      <td>IND</td>\n",
       "      <td>W</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1485.8804</td>\n",
       "      <td>1524.3203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-11-05</td>\n",
       "      <td>Away</td>\n",
       "      <td>SAS</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-11-07</td>\n",
       "      <td>Away</td>\n",
       "      <td>CHO</td>\n",
       "      <td>L</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795333</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>47.333333</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-11-08</td>\n",
       "      <td>Home</td>\n",
       "      <td>NYK</td>\n",
       "      <td>W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781750</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>1489.2977</td>\n",
       "      <td>1491.1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ATL</td>\n",
       "      <td>6</td>\n",
       "      <td>2014-11-10</td>\n",
       "      <td>Away</td>\n",
       "      <td>NYK</td>\n",
       "      <td>W</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770800</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>47.400000</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ATL</td>\n",
       "      <td>7</td>\n",
       "      <td>2014-11-12</td>\n",
       "      <td>Home</td>\n",
       "      <td>UTA</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>12.166667</td>\n",
       "      <td>46.166667</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>24.666667</td>\n",
       "      <td>1505.4128</td>\n",
       "      <td>1383.9275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ATL</td>\n",
       "      <td>8</td>\n",
       "      <td>2014-11-14</td>\n",
       "      <td>Home</td>\n",
       "      <td>MIA</td>\n",
       "      <td>W</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705286</td>\n",
       "      <td>11.571429</td>\n",
       "      <td>43.857143</td>\n",
       "      <td>26.285714</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.571429</td>\n",
       "      <td>15.142857</td>\n",
       "      <td>23.571429</td>\n",
       "      <td>1507.4873</td>\n",
       "      <td>1575.7856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ATL</td>\n",
       "      <td>9</td>\n",
       "      <td>2014-11-15</td>\n",
       "      <td>Away</td>\n",
       "      <td>CLE</td>\n",
       "      <td>L</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.732125</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>42.375000</td>\n",
       "      <td>26.375000</td>\n",
       "      <td>7.375000</td>\n",
       "      <td>6.125000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>23.125000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ATL</td>\n",
       "      <td>10</td>\n",
       "      <td>2014-11-18</td>\n",
       "      <td>Home</td>\n",
       "      <td>LAL</td>\n",
       "      <td>L</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736222</td>\n",
       "      <td>10.888889</td>\n",
       "      <td>42.888889</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>7.888889</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>14.777778</td>\n",
       "      <td>22.111111</td>\n",
       "      <td>1499.0448</td>\n",
       "      <td>1383.3768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Team  Game       Date  Home Opponent WINorLOSS  wins  home_game  opp_wins  \\\n",
       "0  ATL     1 2014-10-29  Away      TOR         L   0.0          1       0.0   \n",
       "1  ATL     2 2014-11-01  Home      IND         W   1.0          2       1.0   \n",
       "2  ATL     3 2014-11-05  Away      SAS         L   1.0          3       1.0   \n",
       "3  ATL     4 2014-11-07  Away      CHO         L   1.0          4       1.0   \n",
       "4  ATL     5 2014-11-08  Home      NYK         W   2.0          5       2.0   \n",
       "5  ATL     6 2014-11-10  Away      NYK         W   3.0          6       3.0   \n",
       "6  ATL     7 2014-11-12  Home      UTA         W   4.0          7       4.0   \n",
       "7  ATL     8 2014-11-14  Home      MIA         W   5.0          8       5.0   \n",
       "8  ATL     9 2014-11-15  Away      CLE         L   5.0          9       5.0   \n",
       "9  ATL    10 2014-11-18  Home      LAL         L   5.0         10       5.0   \n",
       "\n",
       "   opp_games  ...  Opp.FreeThrows.  Opp.OffRebounds  Opp.TotalRebounds  \\\n",
       "0          1  ...              NaN              NaN                NaN   \n",
       "1          2  ...         0.818000        16.000000          48.000000   \n",
       "2          3  ...         0.837500        13.500000          46.000000   \n",
       "3          4  ...         0.795333        12.666667          47.333333   \n",
       "4          5  ...         0.781750        12.250000          48.250000   \n",
       "5          6  ...         0.770800        12.400000          47.400000   \n",
       "6          7  ...         0.739500        12.166667          46.166667   \n",
       "7          8  ...         0.705286        11.571429          43.857143   \n",
       "8          9  ...         0.732125        10.750000          42.375000   \n",
       "9         10  ...         0.736222        10.888889          42.888889   \n",
       "\n",
       "   Opp.Assists  Opp.Steals  Opp.Blocks  Opp.Turnovers  Opp.TotalFouls  \\\n",
       "0          NaN         NaN         NaN            NaN             NaN   \n",
       "1    26.000000   13.000000    9.000000       9.000000       22.000000   \n",
       "2    25.500000    9.000000    7.000000      13.500000       24.000000   \n",
       "3    25.333333    8.333333    7.666667      15.333333       21.000000   \n",
       "4    26.750000    7.750000    7.500000      16.250000       23.250000   \n",
       "5    26.600000    6.600000    7.200000      16.000000       24.400000   \n",
       "6    26.000000    6.166667    6.333333      15.833333       24.666667   \n",
       "7    26.285714    7.000000    6.571429      15.142857       23.571429   \n",
       "8    26.375000    7.375000    6.125000      15.000000       23.125000   \n",
       "9    27.777778    7.888889    5.666667      14.777778       22.111111   \n",
       "\n",
       "         elo    elo_opp  \n",
       "0        NaN        NaN  \n",
       "1  1485.8804  1524.3203  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4  1489.2977  1491.1520  \n",
       "5        NaN        NaN  \n",
       "6  1505.4128  1383.9275  \n",
       "7  1507.4873  1575.7856  \n",
       "8        NaN        NaN  \n",
       "9  1499.0448  1383.3768  \n",
       "\n",
       "[10 rows x 46 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove a win/game\n",
    "A quick note - we need to remove a win from each of the `wins` and `opp_wins` columns. Why? As of right now, it gives us where the team stood *after* that game but to make predictions, we need to know how the team was doing heading *into* the game. If we left it as is, we would be predicting for a game while using data that already included how the game actually went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:46.305316Z",
     "start_time": "2020-02-19T14:10:46.290558Z"
    }
   },
   "outputs": [],
   "source": [
    "# We need to also account for cases when wins = 0, in which case we leave as it is.\n",
    "nba['wins'] = nba['wins'].apply(lambda x: x-1 if x > 0 else x)\n",
    "nba['opp_wins'] = nba['opp_wins'].apply(lambda x: x-1 if x > 0 else x)\n",
    "nba['home_game'] = nba['home_game'].apply(lambda x: x-1 if x > 0 else x)\n",
    "nba['opp_games'] = nba['opp_games'].apply(lambda x: x-1 if x > 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:46.340480Z",
     "start_time": "2020-02-19T14:10:46.306620Z"
    }
   },
   "outputs": [],
   "source": [
    "display(nba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:56:09.673616Z",
     "start_time": "2020-02-15T14:56:09.630989Z"
    }
   },
   "source": [
    "#### Create winning pct column\n",
    "Get the team's current winning pct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:46.345693Z",
     "start_time": "2020-02-19T14:10:46.341845Z"
    }
   },
   "outputs": [],
   "source": [
    "nba['win_pct'] = nba['wins'] / nba['home_game']\n",
    "nba['opp_win_pct'] = nba['opp_wins'] / nba['opp_games']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:46.350133Z",
     "start_time": "2020-02-19T14:10:46.346863Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns\n",
    "We can drop some of the columns we no longer will need. These columns were only created for getting to a different result, such as creating `win_loss` to get the cumulative `wins`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:46.385669Z",
     "start_time": "2020-02-19T14:10:46.351158Z"
    }
   },
   "outputs": [],
   "source": [
    "nba = nba.drop(columns = ['home_game', 'opp_games'])\n",
    "display(nba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a \"final\" dataset to use. Future feature engineering could be done to include a handful of other things, such as:\n",
    "* Travel distance for team\n",
    "* If a team had to change timezones\n",
    "* How many games back-to-back a team had played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:46.393692Z",
     "start_time": "2020-02-19T14:10:46.387007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally we drop the columns with NaN for either team's winning pct because we need games with data on both teams\n",
    "nba = nba[(nba['win_pct'].notna()) & (nba['opp_win_pct'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:46.409770Z",
     "start_time": "2020-02-19T14:10:46.395086Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print final data info\n",
    "nba.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "Some brief exploration of the data now that it has been properly formatted and is in a usable situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:48.081090Z",
     "start_time": "2020-02-19T14:10:46.411159Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:48.085194Z",
     "start_time": "2020-02-19T14:10:48.082751Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pandas_profiling.ProfileReport(nba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "The aim of this is to predict whether or not a team will win or loss. This is found in the `WINorLOSS` column for each game. We have a total of four seasons worth of data and can use all of ths information available since we \"backlogged\" the season averages to not include the game they are from. This means we are attempting to predict the winner of a game based on the averages of the Home and Away teams and their respective winning percentages heading into the game. Let's start with a basic logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:48.090865Z",
     "start_time": "2020-02-19T14:10:48.086890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting WINorLOSS into a numeric column\n",
    "nba['WINorLOSS'] = np.where(nba['WINorLOSS'] == 'W', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do logistic regression, we need to convert all of our categorical variables to 'dummy' variables using the `get_dummies` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:48.128598Z",
     "start_time": "2020-02-19T14:10:48.095506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dummy data\n",
    "nba = nba.drop(columns = {'Date'}) # Remove date column - not relevant for predictions\n",
    "#nba_temp = nba.set_index('Team') # Set the index to the team we are predicting for (preserving original data)\n",
    "nba_dummies = pd.get_dummies(nba, drop_first=True) # Drop first to avoid multicollinearity\n",
    "nba_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:48.136090Z",
     "start_time": "2020-02-19T14:10:48.131127Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba_dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `scikit learn` library, we will now split our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:48.143560Z",
     "start_time": "2020-02-19T14:10:48.137788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup temporary dataframes for features and labels\n",
    "X = nba_dummies.drop(columns = ['WINorLOSS'])\n",
    "y = nba_dummies['WINorLOSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:48.154351Z",
     "start_time": "2020-02-19T14:10:48.145224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:48.159563Z",
     "start_time": "2020-02-19T14:10:48.155759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out dims for each data set\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the data\n",
    "Need to standardize the data before building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:10:48.177156Z",
     "start_time": "2020-02-19T14:10:48.161383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:45:26.784210Z",
     "start_time": "2020-02-19T14:10:48.178720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using LogisticRegressionCV function for cross validation\n",
    "# Define regularization parameters\n",
    "#reg_params = np.arange(0.5, 0.001, -0.001)\n",
    "# Create & fit the CV version of Logistic Regression\n",
    "logRegCVSD = LogisticRegressionCV(Cs=500, penalty='l1', cv=5, solver='liblinear')\n",
    "logRegCVSD.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:45:26.789847Z",
     "start_time": "2020-02-19T14:45:26.785877Z"
    }
   },
   "outputs": [],
   "source": [
    "logRegCVSD.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:45:27.087066Z",
     "start_time": "2020-02-19T14:45:26.791562Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(30, 9))\n",
    "axs.plot(logRegCVSD.Cs_, logRegCVSD.scores_[1].mean(axis=0), marker='o')\n",
    "axs.plot(logRegCVSD.C_, logRegCVSD.scores_[1].mean(axis=0).max(), marker='x',\n",
    "         markersize=15)\n",
    "# axs.set_xscale('log')\n",
    "axs.set_title('')\n",
    "axs.set_xlabel('C Values')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.grid()\n",
    "\n",
    "# print max accuracy\n",
    "print(logRegCVSD.scores_[1].mean(axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:45:27.095916Z",
     "start_time": "2020-02-19T14:45:27.088288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy of the model\n",
    "# Make predictions using the training data\n",
    "train_preds = logRegCVSD.predict(X_train)\n",
    "# Score those predictions\n",
    "train_acc = accuracy_score(train_preds, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "test_preds = logRegCVSD.predict(X_test)\n",
    "# Score those predictions\n",
    "test_acc = accuracy_score(test_preds, y_test)\n",
    "\n",
    "# Print out the accuracy for each\n",
    "print(\"Train Accuracy: {:.2f}%\".format(train_acc*100))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:45:27.309471Z",
     "start_time": "2020-02-19T14:45:27.097483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "clf = DecisionTreeClassifier(random_state=7, min_samples_leaf=25)\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find accuracy for train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:45:27.320359Z",
     "start_time": "2020-02-19T14:45:27.310993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the accuracy on the training data\n",
    "train_data_accuracy = accuracy_score(clf.predict(X_train), y_train)\n",
    "\n",
    "# Find the accuracy on the testing data\n",
    "test_data_accuracy = accuracy_score(clf.predict(X_test) , y_test)\n",
    "\n",
    "print('The accuracy on the training data is {:.0f}%'.format(train_data_accuracy*100))\n",
    "print('The accuracy on the testing data is {:.0f}%'.format(test_data_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree model using GridSearch approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:48:13.583075Z",
     "start_time": "2020-02-19T14:45:27.322134Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Redo tree but with GridSearchCV for fine-tuning the model\n",
    "clf = DecisionTreeClassifier(random_state=7)\n",
    "# Create a dictionary of parameters & ranges to test against\n",
    "parameters = {'min_samples_leaf': range(50,500, 50), 'max_depth': range(3,50)}\n",
    "# Create GridSearchCV\n",
    "gsCV = GridSearchCV(clf, parameters, cv=5, return_train_score=True)\n",
    "# Train the model\n",
    "gsCV.fit(X_train, y_train)\n",
    "# Get the results of the grid search approach\n",
    "grid_results = pd.DataFrame(gsCV.cv_results_)\n",
    "# Order the results on best test score and print out the top ten\n",
    "grid_results.sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Approach\n",
    "Attempting to build a simple neural network using Tensorflow & Keras framework for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:33:41.435008Z",
     "start_time": "2020-02-19T15:33:41.431720Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, LeakyReLU\n",
    "from keras.callbacks import TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T17:52:46.010251Z",
     "start_time": "2020-02-19T17:52:46.007206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data has already been preprocessed\n",
    "# Setup early stopping\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3)\n",
    "\n",
    "# Define input shape\n",
    "input_shape = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T17:52:48.562754Z",
     "start_time": "2020-02-19T17:52:48.482854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(1000, input_shape=(69,), activation='relu'),\n",
    "  tf.keras.layers.Dense(300, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T17:52:51.347872Z",
     "start_time": "2020-02-19T17:52:51.305338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T17:53:28.577811Z",
     "start_time": "2020-02-19T17:53:16.157588Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.3, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T17:53:32.420332Z",
     "start_time": "2020-02-19T17:53:32.278999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:24:21.856811Z",
     "start_time": "2020-02-19T18:01:22.512178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attempt to iteratively repeat this process\n",
    "# Define vector to hold test scores\n",
    "test_scores = []\n",
    "\n",
    "# Loop through criteria in two different loops\n",
    "for i in range(100, 500, 10):\n",
    "    for j in range(50, 150, 10):\n",
    "        # Build the model\n",
    "        model = tf.keras.models.Sequential([\n",
    "          tf.keras.layers.Dense(i, input_shape=(69,), activation='relu'),\n",
    "          tf.keras.layers.Dense(j, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.2),\n",
    "          tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.3, \n",
    "                  callbacks=[callback])\n",
    "        results = model.evaluate(X_test, y_test, verbose=2)\n",
    "        test_scores.append(results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
